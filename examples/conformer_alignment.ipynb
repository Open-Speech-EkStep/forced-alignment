{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "449993fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.utils import model_utils\n",
    "from nemo.collections.asr.models import ASRModel\n",
    "from nemo.collections.asr.models.ctc_models import EncDecCTCModel\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import ctc_segmentation as cs\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4befde51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-10-21 02:00:03 nemo_logging:349] /home/anirudh/miniconda3/envs/alignment/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "      return torch._C._cuda_getDeviceCount() > 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-10-21 02:00:04 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-10-21 02:00:04 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: ../../data/hindi_normalized/filtered_hindi_v1_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 24\n",
      "    shuffle: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 30\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: 8\n",
      "    \n",
      "[NeMo W 2022-10-21 02:00:04 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /root/ekstep/nemo_exp/vakyansh-nemo-experimentation/data/hindi_normalized/stt_valid_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    max_duration: 30\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2022-10-21 02:00:04 modelPT:155] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: ../../data/tarini_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-10-21 02:00:04 features:225] PADDING: 0\n",
      "[NeMo I 2022-10-21 02:00:06 save_restore_connector:243] Model EncDecCTCModelBPE was successfully restored from /home/anirudh/Desktop/forced-alignment/models/nemo/hindi/Conformer-CTC-BPE-Large.nemo.\n"
     ]
    }
   ],
   "source": [
    "model_cfg = ASRModel.restore_from(restore_path='../models/nemo/hindi/Conformer-CTC-BPE-Large.nemo',\n",
    "                                  return_config=True)\n",
    "classpath = model_cfg.target\n",
    "imported_class = model_utils.import_class_by_path(classpath)\n",
    "asr_model = imported_class.restore_from(restore_path='../models/nemo/hindi/Conformer-CTC-BPE-Large.nemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ac01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#True if model is BPE\n",
    "bpe_model = isinstance(asr_model, nemo_asr.models.EncDecCTCModelBPE)\n",
    "if bpe_model:\n",
    "    tokenizer = asr_model.tokenizer\n",
    "else:\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879fa62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ε',\n",
       " '<unk>',\n",
       " 'ा',\n",
       " 'र',\n",
       " 'ी',\n",
       " 'े',\n",
       " 'न',\n",
       " 'ि',\n",
       " 'क',\n",
       " '▁',\n",
       " 'त',\n",
       " '्',\n",
       " '▁स',\n",
       " 'ल',\n",
       " 'ं',\n",
       " 'स',\n",
       " 'म',\n",
       " '▁है',\n",
       " '▁क',\n",
       " 'ु',\n",
       " 'ह',\n",
       " 'ग',\n",
       " 'य',\n",
       " 'ो',\n",
       " '▁ब',\n",
       " 'द',\n",
       " 'व',\n",
       " '▁म',\n",
       " '▁के',\n",
       " '्य',\n",
       " '▁प',\n",
       " '▁अ',\n",
       " '▁में',\n",
       " 'प',\n",
       " '▁ज',\n",
       " 'ू',\n",
       " '▁द',\n",
       " 'ज',\n",
       " 'ब',\n",
       " '▁कर',\n",
       " '▁व',\n",
       " 'श',\n",
       " 'ने',\n",
       " 'च',\n",
       " '▁आ',\n",
       " '▁ह',\n",
       " '▁को',\n",
       " 'ट',\n",
       " 'ता',\n",
       " 'ों',\n",
       " '▁और',\n",
       " '▁का',\n",
       " 'ध',\n",
       " '▁की',\n",
       " '▁हो',\n",
       " '्र',\n",
       " 'ए',\n",
       " '▁से',\n",
       " '▁कि',\n",
       " 'थ',\n",
       " '▁हैं',\n",
       " '▁न',\n",
       " 'ते',\n",
       " '▁हम',\n",
       " 'ना',\n",
       " '▁प्र',\n",
       " '▁ल',\n",
       " '▁ग',\n",
       " '▁उ',\n",
       " 'ें',\n",
       " '▁भ',\n",
       " 'ै',\n",
       " 'ई',\n",
       " '▁त',\n",
       " '▁च',\n",
       " '▁इस',\n",
       " 'भ',\n",
       " '▁भी',\n",
       " '▁पर',\n",
       " '▁तो',\n",
       " '▁र',\n",
       " 'ण',\n",
       " '▁उस',\n",
       " 'ड',\n",
       " '▁रह',\n",
       " '▁जा',\n",
       " '▁जो',\n",
       " '▁नहीं',\n",
       " '▁आप',\n",
       " '▁श',\n",
       " 'ड़',\n",
       " 'ष',\n",
       " '▁एक',\n",
       " 'िया',\n",
       " 'ख',\n",
       " '्व',\n",
       " '्ट',\n",
       " '▁यह',\n",
       " '▁वि',\n",
       " 'के',\n",
       " 'ित',\n",
       " '▁इ',\n",
       " 'छ',\n",
       " 'फ',\n",
       " '़',\n",
       " 'ँ',\n",
       " 'ौ',\n",
       " 'ठ',\n",
       " 'झ',\n",
       " 'ॉ',\n",
       " 'इ',\n",
       " 'ओ',\n",
       " 'ऐ',\n",
       " 'ढ',\n",
       " 'घ',\n",
       " 'आ',\n",
       " 'ञ',\n",
       " 'ऊ',\n",
       " 'ऑ',\n",
       " 'उ',\n",
       " 'अ',\n",
       " 'ः',\n",
       " 'औ',\n",
       " 'ॅ',\n",
       " 'ऱ',\n",
       " 'ऩ',\n",
       " 'ऋ',\n",
       " 'ङ',\n",
       " 'ृ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\"ε\"] + list(asr_model.cfg.decoder.vocabulary)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880b0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_file = 'sample.txt'\n",
    "sample_rate, signal = wav.read('sample.wav')\n",
    "original_duration = len(signal) / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72cf07cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d79490608df4e34b9878fb4fab4e3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-10-21 02:00:09 nemo_logging:349] /home/anirudh/miniconda3/envs/alignment/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "      warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "log_probs = asr_model.transcribe(paths2audio_files=['sample.wav'], batch_size=1, logprobs=True)[0]\n",
    "blank_col = log_probs[:, -1].reshape((log_probs.shape[0], 1))\n",
    "log_probs = np.concatenate((blank_col, log_probs[:, :-1]), axis=1)\n",
    "index_duration = len(signal) / log_probs.shape[0] / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d674e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['क्या सेंट मैरीस की एयर क्वालिटी घातक है ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(segment_file, \"r\") as f:\n",
    "    text = f.read().splitlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dca835c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['क्वालिटी']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df7b8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cs.CtcSegmentationParameters()\n",
    "config.char_list = vocabulary\n",
    "config.min_window_size = 4000\n",
    "config.index_duration = index_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f242930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_tokenized_text_for_bpe_model(text: List[str], tokenizer, vocabulary: List[str], blank_idx: int = 0):\n",
    "    \"\"\" Creates a transition matrix for BPE-based models\"\"\"\n",
    "    space_idx = vocabulary.index(\"▁\")\n",
    "    ground_truth_mat = [[-1, -1]]\n",
    "    utt_begin_indices = []\n",
    "    for uttr in text:\n",
    "        ground_truth_mat += [[blank_idx, space_idx]]\n",
    "        utt_begin_indices.append(len(ground_truth_mat))\n",
    "        token_ids = tokenizer.text_to_ids(uttr)\n",
    "        # blank token is moved from the last to the first (0) position in the vocabulary\n",
    "        token_ids = [idx + 1 for idx in token_ids]\n",
    "        ground_truth_mat += [[t, -1] for t in token_ids]\n",
    "\n",
    "    utt_begin_indices.append(len(ground_truth_mat))\n",
    "    ground_truth_mat += [[blank_idx, space_idx]]\n",
    "    ground_truth_mat = np.array(ground_truth_mat, np.int64)\n",
    "    return ground_truth_mat, utt_begin_indices\n",
    "\n",
    "def _print(ground_truth_mat, vocabulary, limit=20):\n",
    "    \"\"\"Prints transition matrix\"\"\"\n",
    "    chars = []\n",
    "    for row in ground_truth_mat:\n",
    "        chars.append([])\n",
    "        for ch_id in row:\n",
    "            if ch_id != -1:\n",
    "                chars[-1].append(vocabulary[int(ch_id)])\n",
    "\n",
    "    for x in chars[:limit]:\n",
    "        print(\"unknown\")\n",
    "        #logging.debug(x)\n",
    "\n",
    "def determine_utterance_segments(config, utt_begin_indices, char_probs, timings, text, char_list):\n",
    "    \"\"\"Utterance-wise alignments from char-wise alignments.\n",
    "    Adapted from https://github.com/lumaku/ctc-segmentation\n",
    "    Args:\n",
    "        config: an instance of CtcSegmentationParameters\n",
    "        utt_begin_indices: list of time indices of utterance start\n",
    "        char_probs:  character positioned probabilities obtained from backtracking\n",
    "        timings: mapping of time indices to seconds\n",
    "        text: list of utterances\n",
    "    Return:\n",
    "        segments, a list of: utterance start and end [s], and its confidence score\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    min_prob = np.float64(-10000000000.0)\n",
    "    for i in tqdm(range(len(text))):\n",
    "        start = _compute_time(utt_begin_indices[i], \"begin\", timings)\n",
    "        end = _compute_time(utt_begin_indices[i + 1], \"end\", timings)\n",
    "\n",
    "        start_t = start / config.index_duration_in_seconds\n",
    "        start_t_floor = math.floor(start_t)\n",
    "\n",
    "        # look for the left most blank symbol and split in the middle to fix start utterance segmentation\n",
    "        if char_list[start_t_floor] == config.char_list[config.blank]:\n",
    "            start_blank = None\n",
    "            j = start_t_floor - 1\n",
    "            while char_list[j] == config.char_list[config.blank] and j > start_t_floor - 20:\n",
    "                start_blank = j\n",
    "                j -= 1\n",
    "            if start_blank:\n",
    "                start_t = int(round(start_blank + (start_t_floor - start_blank) / 2))\n",
    "            else:\n",
    "                start_t = start_t_floor\n",
    "            start = start_t * config.index_duration_in_seconds\n",
    "\n",
    "        else:\n",
    "            start_t = int(round(start_t))\n",
    "\n",
    "        end_t = int(round(end / config.index_duration_in_seconds))\n",
    "\n",
    "        # Compute confidence score by using the min mean probability after splitting into segments of L frames\n",
    "        n = config.score_min_mean_over_L\n",
    "        if end_t <= start_t:\n",
    "            min_avg = min_prob\n",
    "        elif end_t - start_t <= n:\n",
    "            min_avg = char_probs[start_t:end_t].mean()\n",
    "        else:\n",
    "            min_avg = np.float64(0.0)\n",
    "            for t in range(start_t, end_t - n):\n",
    "                min_avg = min(min_avg, char_probs[t : t + n].mean())\n",
    "        segments.append((start, end, min_avg))\n",
    "    return segments\n",
    "\n",
    "def _compute_time(index, align_type, timings):\n",
    "    \"\"\"Compute start and end time of utterance.\n",
    "    Adapted from https://github.com/lumaku/ctc-segmentation\n",
    "    Args:\n",
    "        index:  frame index value\n",
    "        align_type:  one of [\"begin\", \"end\"]\n",
    "    Return:\n",
    "        start/end time of utterance in seconds\n",
    "    \"\"\"\n",
    "    middle = (timings[index] + timings[index - 1]) / 2\n",
    "    if align_type == \"begin\":\n",
    "        return max(timings[index + 1] - 0.5, middle)\n",
    "    elif align_type == \"end\":\n",
    "        return min(timings[index - 1] + 0.5, middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9512594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bpe_model:\n",
    "    ground_truth_mat, utt_begin_indices = _prepare_tokenized_text_for_bpe_model(text, tokenizer, vocabulary, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3f5f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.blank = 0\n",
    "timings, char_probs, char_list = cs.ctc_segmentation(config, log_probs, ground_truth_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a12b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1945.41it/s]\n"
     ]
    }
   ],
   "source": [
    "segments = determine_utterance_segments(config, utt_begin_indices, char_probs, timings, text, char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c86ca2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.1514285714285717, 2.968174603174603, -1.1160802831427645)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "437f1437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 9]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_begin_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f230551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क्वालिटी\n",
      "(2.1514285714285717, 2.968174603174603, -1.1160802831427645)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i, (word, segment) in enumerate(zip(text, segments)):\n",
    "    print(word)\n",
    "    print(segment)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "05708d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(wav, start, end):\n",
    "    frames = AudioSegment.from_wav(wav)\n",
    "    s = start*1000 + 500\n",
    "    print(s)\n",
    "    e = int(end*1000) + 50\n",
    "    print(e)\n",
    "    return frames[s:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "efd9e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651.0\n",
      "3018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <audio controls>\n",
       "                        <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\"/>\n",
       "                        Your browser does not support the audio element.\n",
       "                    </audio>\n",
       "                  "
      ],
      "text/plain": [
       "<pydub.audio_segment.AudioSegment at 0x7f7e3f6bdd90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped = clip('sample.wav', 2.151, 2.968)\n",
    "clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33432648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
